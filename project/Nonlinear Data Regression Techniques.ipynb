{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972ff741-68df-4533-a75b-913dfa8976fd",
   "metadata": {},
   "source": [
    "# 監督式學習：回歸分析\n",
    "# Supervised Learning: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e09d13-a97c-437c-a912-1c1d23250734",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcd568-6ea8-4b7e-a417-a21199585134",
   "metadata": {},
   "source": [
    "單變量與多變量的線性回歸後，我們能夠對線性趨勢的資料做預測。<br>\n",
    "不過，生活中的數據並不總是「線性」的，例如股市波動、交通流量等等。<br>\n",
    "後面會講，非線性資料要怎麼處理<br>\n",
    "After mastering the $unary$ and $multiple$ forms of linear regression, we can make regression predictions for some data with linear distribution trends. However, there are often data in the life that are not so \"linear\", such as fluctuations in the stock market, traffic flow etc. Later I will introduce the process of this kind of non-linearly distributed data. Methods for the same will be described in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a49aa3-c169-4e18-bf9f-e0920d0d266f",
   "metadata": {},
   "source": [
    "多項式\n",
    "Polynomial<br>\n",
    "多項式擬合\n",
    "Polynomial fitting<br>\n",
    "最小平方法\n",
    "Least squares method<br>\n",
    "過擬合問題\n",
    "Overfitting<br>\n",
    "資料集切分\n",
    "Dataset partition<br>\n",
    "最佳模型選擇\n",
    "Selection of optimal model<br>\n",
    "使用 scikit-learn 實作多項式回歸\n",
    "`scikit-learn` implementations of polynomial regression prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c80db-1612-4488-ad8f-b364b1021aad",
   "metadata": {},
   "source": [
    "## 多項式回歸介紹\n",
    "## Introduction to Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac4c46-d332-434e-826b-d502ba20a54c",
   "metadata": {},
   "source": [
    "在線性回歸中，我們透過建立自變數 x 的線性方程式來擬合資料；而在非線性回歸中，則必須建立自變數與應變數之間的非線性關係。直觀地說，用來擬合資料的「直線」會改為「曲線」。<br>\n",
    "In linear regression, we fit the data by establishing a linear equation of an independent variable $x$. While in non-linear regression, it is necessary to establish a non-linear relationship between independent variable and dependent variable. Intuitively, the straight line that fits the data becomes a \"curve\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2863a-12f5-441c-9fb6-964283170a19",
   "metadata": {},
   "source": [
    "As shown in the figure below, the points are from the change data of the population in a certain area. If we use linear variance to fit the data, then there is a \"visually visible\" error. For such a data distribution, using a curve to fit is much more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087e763-0698-4faf-bebd-981faaad7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 1. 生成模擬資料\n",
    "rng = np.random.RandomState(0)\n",
    "x = np.sort(rng.rand(20) * 10)          # 0–10 之間的自變數\n",
    "y = 0.5 * x**2 - x + 3 + rng.randn(20) * 3  # 加入雜訊的二次關係\n",
    "\n",
    "# 2. 線性回歸模型\n",
    "lin_reg = LinearRegression().fit(x.reshape(-1, 1), y)\n",
    "x_plot = np.linspace(x.min(), x.max(), 100)\n",
    "y_lin = lin_reg.predict(x_plot.reshape(-1, 1))\n",
    "\n",
    "# 3. 二次多項式回歸模型\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree=2, include_bias=False),\n",
    "                           LinearRegression())\n",
    "poly_model.fit(x.reshape(-1, 1), y)\n",
    "y_poly = poly_model.predict(x_plot.reshape(-1, 1))\n",
    "\n",
    "# 4. 繪圖\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# 左圖：線性回歸\n",
    "axes[0].scatter(x, y)\n",
    "axes[0].plot(x_plot, y_lin)\n",
    "axes[0].set_title(\"linear regression\")\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "axes[0].set_xlim(x.min() - 1, x.max() + 1)\n",
    "\n",
    "# 右圖：非線性（二次）回歸\n",
    "axes[1].scatter(x, y)\n",
    "axes[1].plot(x_plot, y_poly)\n",
    "axes[1].set_title(\"non-linear regression\")\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_xlim(x.min() - 1, x.max() + 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f509a-4d9d-4a91-9ee1-a102932e8c67",
   "metadata": {},
   "source": [
    "For non-linear regression problems, the simplest and most common method is the \"polynomial regression\", to be explained in this experiment. The polynomial is an early learning concept. We cite its definition by [Wikipedia](https://en.wikipedia.org/wiki/Polynomial) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ff136-17d9-4966-985c-70dab2d895d2",
   "metadata": {},
   "source": [
    ">*In mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction and multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate $x$ is $x^2-3x+4$. An example in three variables is $x^3-2xyz^2+2yz+1$.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c2ad5-00ca-4fed-b8e3-ccdb4c9803ae",
   "metadata": {},
   "source": [
    "##  Polynomial Regression Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cbedb-d4c9-45d9-b46f-bd4e301cc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4, 8, 12, 25, 32, 43, 58, 63, 69, 79]\n",
    "y = [20, 33, 50, 56, 42, 31, 33, 46, 65, 75]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b679dc-5dad-46cc-8809-5342f6acc10b",
   "metadata": {},
   "source": [
    "總共有十組樣本資料，分別對應於橫座標（x）與縱座標（y）。接著我們使用 Matplotlib 繪製這些資料，並觀察其趨勢。<br>\n",
    "There are total ten sets of sample data, corresponding to abscissa and ordinate. We then draw the data through Matplotlib and show its trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0f364-9c6e-4d25-bf06-f3f52e0fb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74001a2-add3-4918-aba6-d7e1a0c85a29",
   "metadata": {},
   "source": [
    "### Quadratic Polynomial Fitting\n",
    "### 二次多項式擬合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885fe46-83a3-401a-86f5-30054c1bc449",
   "metadata": {},
   "source": [
    "The above scatter data needs to be fitted by a polynomial. To achieve this, a standard unary high-degree polynomial function like the following one is required:<br>\n",
    "上方散佈點資料需要以多項式來擬合。為此，我們需使用如下的標準單變數高次多項式函式："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e61a2f-fb00-4f37-a468-76b374e46787",
   "metadata": {},
   "source": [
    "$$ y(x, w) = w_0 + w_1x + w_2x^2 +...+w_mx^m = \\sum\\limits_{j=0}^{m}w_jx^j \\tag{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d0597-3e04-442c-81f4-d38af02f7397",
   "metadata": {},
   "source": [
    "Where $m$ represents the order of the polynomial and $x^j$ expresses $x$ raised to the power $j$. $w$ is the coefficient of the polynomial.<br>\n",
    "m：多項式的階數<br>\n",
    "x^j：表示x的j次方"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75ac13-373f-492b-beaa-969d9ed65003",
   "metadata": {},
   "source": [
    "If the value of the polynomial degree $m$ is manually specified, then we only need to determine what the polynomial coefficient $w$ is. For example, assume $m=2$ here. Then the polynomial becomes:\n",
    "$$ y(x, w) = w_0 + w_1x + w_2x^2= \\sum\\limits_{j=0}^{2}w_jx^j \\tag{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373ae4c-8598-4938-80a7-d18161ddc63c",
   "metadata": {},
   "source": [
    "When we determine the value of $w$, we return to what we learned in the previous linear regression experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35e5cb-83e4-4068-bd9f-3d90b9a7d937",
   "metadata": {},
   "source": [
    "First, we construct two functions: a polynomial function for fitting and a loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd7700-c942-41e2-bb92-3c1eeba4a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define a quadratic polynomial function and a loss function\n",
    "\"\"\"\n",
    "def func(p, x):\n",
    "    \"\"\"Define a quadratic polynomial function\n",
    "    \"\"\"\n",
    "    w0, w1, w2 = p\n",
    "    f = w0 + w1*x + w2*x*x\n",
    "    return f\n",
    "\n",
    "def err_func(p, x, y):\n",
    "    \"\"\"Define a loss function\n",
    "    \"\"\"\n",
    "    ret = func(p, x) - y\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0256a1-16af-422a-bcd5-8283aa334a99",
   "metadata": {},
   "source": [
    "Next, we simply initialize three $w$ parameters using the random number method provided by NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f222f7-9410-478d-8960-c7ebdd458f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_init = np.random.randn(3) # Generate 3 random numbers\n",
    "\n",
    "p_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e52c78-aa4c-4bed-a518-9cb5898eebe9",
   "metadata": {},
   "source": [
    "Next is the process of solving for the optimal parameters using the least squares method. For convenience, we use the least squares class provided by SciPy to get the best fitting parameters. Of course, you can solve for the parameters yourself by the least squares formula in the **\"Linear Regression\"** chapter. However, in practice, in order to achieve results quickly, you will often use a ready-made function library like **SciPy**. Let us introduce a method here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0b8b6-1206-4d75-bbdb-426b8ea52287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use the least squares function provided by SciPy\n",
    "\"\"\"\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))\n",
    "\n",
    "print('Fitting Parameters: ', parameters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae344d9-dda6-4ad3-8ed3-41cfa7cab2d7",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">*For the specific use of `scipy.optimize.leastsq()`, you can read [Official Document](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1abd94-bc5a-4e08-8a1b-534dea6e90be",
   "metadata": {},
   "source": [
    "The best fitting parameters $w_0$, $w_1$ and $w_2$ we get here are `3.76893117e+01`, `-2.60474147e-01` and `8.00078082e-03`. That is, our fitted function (retaining two significant digits) is:\n",
    "\n",
    "$$ y(x) = 37 - 0.26*x + 0.0080*x^2 \\tag{3} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c94de-1617-41ea-aee5-b8bc7c845e98",
   "metadata": {},
   "source": [
    "Then we try to draw the fitted figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7cad5-6465-4937-8266-c8802d97fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Draw the fitted figure\n",
    "\"\"\"\n",
    "# Generate the testing points\n",
    "x_temp = np.linspace(0, 80, 10000)\n",
    "\n",
    "# Draw the fitted curve\n",
    "plt.plot(x_temp, func(parameters[0], x_temp), 'r')\n",
    "\n",
    "# Draw the original scatters\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aef764-915a-4ed4-800e-3cffae31ea70",
   "metadata": {},
   "source": [
    "### N-Degree Polynomial Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970235c-51c8-49c5-9faf-a40a484aaf6d",
   "metadata": {},
   "source": [
    "You will find that the results of the `second-degree` polynomial do not properly reflect the trend of the scatter. At this point, we try a polynomial fit of `third degree` or higher. In the following part, we slightly modify the code to implement an `N-degree` polynomial fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06970f-7cb7-48b9-9528-4a1024ceb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"N-degree Polynomial Fitting\n",
    "\"\"\"\n",
    "def fit_func(p, x):\n",
    "    \"\"\"Define an N-degree polynomial function\n",
    "    \"\"\"\n",
    "    f = np.poly1d(p)\n",
    "    return f(x)\n",
    "\n",
    "def err_func(p, x, y):\n",
    "    \"\"\"Define a loss function\n",
    "    \"\"\"\n",
    "    ret = fit_func(p, x) - y\n",
    "    return ret\n",
    "\n",
    "def n_poly(n):\n",
    "    \"\"\"N-degree polynomial fitting\n",
    "    \"\"\"\n",
    "    p_init = np.random.randn(n) # Generate N random numbers\n",
    "    parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))\n",
    "    return parameters[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940f768-015a-4bd5-8909-7bd3d41975bc",
   "metadata": {},
   "source": [
    "You can use $n=3$ to verify whether the above code is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adee704-6af2-40be-ba0e-151d276201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_poly(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d51e2a-ec4e-4151-8449-cc62bfb88828",
   "metadata": {},
   "source": [
    "The result obtained at this time is consistent with the result of the formula (3), but the order is different. This is because the default mode of the polynomial function `np.poly1d(3)` in NumPy is:\n",
    "\n",
    "$$ y(x) = 0.0080*x^2 - 0.26*x + 37\\tag{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b48ac-cdf1-4f36-9cae-f750874357cc",
   "metadata": {},
   "source": [
    "Now, we plot the fitting result of the `4-`, `5-`, `6-`, `7-` and `8-` `degree` polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a6901-2e70-46ac-b600-da00fdfe2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the fitting result of the 4-, 5-, 6-, 7- and 8- degree polynomials\n",
    "\"\"\"\n",
    "\n",
    "# Generate the testing points\n",
    "x_temp = np.linspace(0, 80, 10000)\n",
    "\n",
    "# Generate the sub-images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "axes[0,0].plot(x_temp, fit_func(n_poly(4), x_temp), 'r')\n",
    "axes[0,0].scatter(x, y)\n",
    "axes[0,0].set_title(\"m = 4\")\n",
    "\n",
    "axes[0,1].plot(x_temp, fit_func(n_poly(5), x_temp), 'r')\n",
    "axes[0,1].scatter(x, y)\n",
    "axes[0,1].set_title(\"m = 5\")\n",
    "\n",
    "axes[0,2].plot(x_temp, fit_func(n_poly(6), x_temp), 'r')\n",
    "axes[0,2].scatter(x, y)\n",
    "axes[0,2].set_title(\"m = 6\")\n",
    "\n",
    "axes[1,0].plot(x_temp, fit_func(n_poly(7), x_temp), 'r')\n",
    "axes[1,0].scatter(x, y)\n",
    "axes[1,0].set_title(\"m = 7\")\n",
    "\n",
    "axes[1,1].plot(x_temp, fit_func(n_poly(8), x_temp), 'r')\n",
    "axes[1,1].scatter(x, y)\n",
    "axes[1,1].set_title(\"m = 8\")\n",
    "\n",
    "axes[1,2].plot(x_temp, fit_func(n_poly(9), x_temp), 'r')\n",
    "axes[1,2].scatter(x, y)\n",
    "axes[1,2].set_title(\"m = 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92405ffa-059c-403e-8b3f-d1385a038c92",
   "metadata": {},
   "source": [
    "As can be seen from the above six figures, when $m=4$ (4th degree polynomial), the fitting result is significantly better than that of $m=3$. However, as the value of $m$ increases to $8$, the curve shows a significant oscillation, which is the overfitting phenomenon mentioned in the linear regression experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d21bc-a238-4d63-b8f0-c9ed028ffd8e",
   "metadata": {},
   "source": [
    "### Polynomial Fitting with `scikit-learn`\n",
    "### 使用 scikit-learn 進行多項式擬合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa452fb-809c-47cc-ae51-5b070e848bb5",
   "metadata": {},
   "source": [
    "Above we have defined the polynomial and implemented the polynomial regression fitting process on our own. In addition, we can also use the polynomial regression method provided by `scikit-learn`. Here, we will use the `sklearn.preprocessing.PolynomialFeatures()` class. The main use of `PolynomialFeatures()` is to generate a polynomial feature matrix. **If you are new to this concept, you may need to read the following carefully:**\n",
    "<br><br>\n",
    "前面我們已經手動定義了多項式，並自己實作了多項式回歸的擬合流程。除此之外，我們也可以使用 scikit-learn 提供的多項式回歸工具。<br><br>\n",
    "這裡我們會使用 sklearn.preprocessing.PolynomialFeatures() 這個類別。<br><br>\n",
    "PolynomialFeatures() 的主要用途是用來產生多項式的特徵矩陣。<br><br>\n",
    "如果你對這個概念還不熟，建議你仔細閱讀以下說明："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c0d92-9e2c-4ffd-9c21-7f74085145ca",
   "metadata": {},
   "source": [
    "For a quadratic polynomial, we know its standard form is: $y(x, w) = w_0 + w_1x + w_2x^2$. However, polynomial regression is equivalent to a special form of linear regression indeed. For example, here we set $x = x_1$ and $x^2 = x_2$. Then the original equation is converted to: $y(x, w) = w_0 + w_1*x_1 + w_2*x_2$, that is, multiple linear regression. This achieves **the conversion between a unary high-degree polynomial to a multivariate one-time polynomial**.<br><br>\n",
    "對於一個二次多項式，我們知道它的標準形式是：<br>\n",
    "$y(x, w) = w_0 + w_1x + w_2x^2$<br>\n",
    "<br>\n",
    "但其實，多項式回歸本質上等同於一種特殊形式的線性回歸。<br>\n",
    "舉例來說，我們可以令：<br>\n",
    " $x = x_1$ ，$x^2 = x_2$<br>\n",
    "<br>\n",
    "那麼原本的公式就會轉換成：<br>\n",
    "$y(x, w) = w_0 + w_1*x_1 + w_2*x_2$<br>\n",
    "<br>\n",
    "這樣一來，就完成了「將單變量高次多項式轉換為多變量一次多項式」的過程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3a485-ce02-49c8-8638-8b3347bc1b01",
   "metadata": {},
   "source": [
    "For example, for the independent variable vector $X$ and the dependent variable $y$, if we have $X$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60875a8a-53a9-49af-b9d5-46139380cfdd",
   "metadata": {},
   "source": [
    "$$ \\mathbf{X} = \\begin{bmatrix}\n",
    "       2    \\\\[0.3em]\n",
    "       -1 \\\\[0.3em]\n",
    "       3         \n",
    "     \\end{bmatrix} \\tag{5a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e1d46-1780-4114-a238-8bb6268e3ea3",
   "metadata": {},
   "source": [
    "We can fit it through the $y = w_1 x + w_0$ linear regression model. Similarly, for a quadratic polynomial $y(x, w) = w_0 + w_1x + w_2x^2$, if you can get the feature matrix composed of $x = x_1$, $x^2 = x_2$, namely:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cfe0b-9628-4e56-816f-3d86233cfaf7",
   "metadata": {},
   "source": [
    "$$\\mathbf{X} = \\left [ X, X^2 \\right ] = \\begin{bmatrix}\n",
    " 2& 4\\\\ -1\n",
    " & 1\\\\ 3\n",
    " & 9\n",
    "\\end{bmatrix}\n",
    "\\tag{5b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50a89a-6417-4ee6-8667-3338e9268124",
   "metadata": {},
   "source": [
    "Then you can fit it through linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d5d84-0984-423c-a309-90fb1722d6b2",
   "metadata": {},
   "source": [
    "Actually you can manually verify the above results, but what needs to be kept in mind is— **When the polynomial is of high degree, the expression as well as the calculation of the feature matrix will become much more complicated.** For example, the following is a feature matrix expression for a binary quadratic polynomial:<br><br>\n",
    "其實你可以手動驗證上述結果，但需要記住的是——當多項式的階數變高時，不論是表達式還是特徵矩陣的計算都會變得更加複雜。<br><br>\n",
    "例如，下面是針對一個雙變數二次多項式所對應的特徵矩陣表示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34be3e-7eb4-4ad4-b150-a09a76b6accc",
   "metadata": {},
   "source": [
    "$$\\mathbf{X} = \\left [ X_{1}, X_{2}, X_{1}^2, X_{1}X_{2}, X_{2}^2 \\right ]\n",
    "\\tag{5c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e96cb9-33e2-4225-a385-c86bf6173c37",
   "metadata": {},
   "source": [
    "Fortunately, in `scikit-learn`, we can automatically generate a polynomial feature matrix through the `PolynomialFeatures()` class. The default parameters and the common parameters of the `PolynomialFeatures()` class are defined as follows:<br><br>\n",
    "所幸在 scikit-learn 中，我們可以透過 PolynomialFeatures() 類別自動產生多項式的特徵矩陣。<br><br>\n",
    "這個類別的預設參數與常用參數如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e7e2b-8a73-4735-8ae1-3527388fdd73",
   "metadata": {},
   "source": [
    "```python\n",
    "sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\n",
    "```\n",
    "- `degree`: 2 by default. The degree of the polynomial features.\n",
    "- `interaction_only`: False by default. If True, only interaction features are produced.\n",
    "- `include_bias`: If True (default), then include a bias column.\n",
    "<br><br>\n",
    "- `degree`: 預設為 2。表示多項式特徵的階數。\n",
    "- `interaction_only`: 預設為 False。若設為 True，則只生成交互作用項（不包含每個變數的次方項）。\n",
    "- `include_bias`: 預設為 True。若為 True，則會在特徵矩陣中加入一列常數偏差項（通常是 1）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad05370-75dd-4e14-8e76-3fa31fff8074",
   "metadata": {},
   "source": [
    "Corresponding to the above eigen vectors, the main use of `PolynomialFeatures()` is to generate a feature matrix as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323e277-db2b-4a86-aa21-2bb23b96f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use PolynomialFeatures() to generate a feature matrix\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X=[2, -1, 3]\n",
    "X_reshape = np.array(X).reshape(len(X), 1) # Transpose\n",
    "PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacab59-418c-4c06-8ce9-5d2a28778a2c",
   "metadata": {},
   "source": [
    "For the matrix in the upper cell, the first column is $X^1$ and the second one is $X^2$. Then we use the polynomial function $y(x, w) = w_0 + w_1*x_1 + w_2*x_2$ to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290d0a3-a6d4-4b3e-89ab-e4817b8d775b",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">*Note: In this lesson, you will see a lot of `reshape` operations, all of which are designed to satisfy the array shape of some class-passed arguments. These operations are necessary in this experiment because the original shape of the data (such as the one-dimensional array above) may not be passed directly to some specific class. But it's not necessary in practice because the raw dataset shape at your hand may support direct input. So, don't be afraid of these `reshape` operations.</div><br>\n",
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">註：在這個課程中，你會看到許多 reshape 操作，這些操作主要是為了配合某些類別在傳入參數時對陣列形狀的要求。這些操作在本實驗中是必要的，因為原始資料的形狀（像是前面的一維陣列）可能無法直接傳入某些類別中。不過在實務上，手邊的資料集形狀可能已經符合要求，所以不一定需要 reshape。因此，別害怕看到這些 reshape 操作。</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d40d6-026b-4598-8576-3c87f3d67a77",
   "metadata": {},
   "source": [
    "Going back to the sample data in *Section 2.1*, the independent variable is $x$ and the dependent variable is $y$. If a quadratic polynomial is adopted, then we use `PolynomialFeatures()` to generate the feature matrix.<br><br>\n",
    "回到 第 2.1 節 的範例資料中，自變數是 x，應變數是 y。若採用二次多項式，我們就可以使用 PolynomialFeatures() 來產生對應的特徵矩陣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f16149-9382-4465-8971-8e5dce85d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use sklearn to generate the feature matrix of a quadratic polynomial\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array(x).reshape(len(x), 1) # Transpose\n",
    "y = np.array(y).reshape(len(y), 1)\n",
    "\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_x = poly_features.fit_transform(x)\n",
    "\n",
    "poly_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e167fe4-5e15-42ec-a00a-a52e93cd14f7",
   "metadata": {},
   "source": [
    "It can be seen that the output result exactly matches our expectations:$\\left [ X, X^2 \\right ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c2b88-a46e-461e-904c-32bed077b5ef",
   "metadata": {},
   "source": [
    "Then we train the linear regression model with `scikit-learn`. The `LinearRegression()` class will be used here. The default parameters and the common parameters of the `LinearRegression()` class are defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18ba10-7a60-4e44-9272-0e2bbc018876",
   "metadata": {},
   "source": [
    "```python\n",
    "sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "```\n",
    "- `fit_intercept`: True by default. Whether to calculate the intercept for this model.\n",
    "- `normalize`: False by default. If True, the regressors will be normalized before regression by subtracting the mean and dividing by the l2-norm.\n",
    "- `copy_X`: If True (default), X will be copied; else it may be overwritten.\n",
    "- `n_jobs`: The number of jobs to use for the computation (1 by default). If -1, all CPU cores are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13170fc5-7b4c-4e65-ae67-274a4b188f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert to linear regression predictions\n",
    "\"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(poly_x, y) # 训练\n",
    "\n",
    "# Obtain the linear regression parameters\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3ad95-3da8-4fcc-a604-78fbad088ee3",
   "metadata": {},
   "source": [
    "You will find that the parameter values obtained here are consistent with equations (3) and (4). To be more intuitive, the fitted figure is also drawn here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274819e9-3778-4913-b28d-39f8dc899630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the fitted figure\n",
    "\"\"\"\n",
    "x_temp = np.array(x_temp).reshape(len(x_temp),1)\n",
    "poly_x_temp = poly_features.fit_transform(x_temp)\n",
    "\n",
    "plt.plot(x_temp, model.predict(poly_x_temp), 'r')\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4f156-c38e-45c9-bada-3cf516b2c29b",
   "metadata": {},
   "source": [
    "Does this figure seem familiar? In fact, it is consistent with the figure below for equation (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc36b0e-5984-4a0d-8356-9447ee6b621f",
   "metadata": {},
   "source": [
    "##  Predictions Using Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293174c-22d2-4661-83c5-2f3746f14b70",
   "metadata": {},
   "source": [
    "Above we learned how to use polynomials to fit the data. Now we will introduce how to use polynomial regression to solve practical prediction problems. In this forecasting experiment, we will use the *World Measles Vaccination Rate* dataset provided by the World Health Organization and UNICEF. The goal is to predict the measles vaccination rate for the corresponding year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3af74-410b-4f9d-802f-62fe952bb4db",
   "metadata": {},
   "source": [
    "### To Load and Preview the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9f149-4e48-4640-b8ff-52cdd73d7104",
   "metadata": {},
   "source": [
    "First we import the *World Measles Vaccination Rate* dataset. The dataset name is: `course-6-vaccine.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465d35b-812f-44a2-9eab-70f5e36b7f9e",
   "metadata": {},
   "source": [
    "!wget http://labfile.oss.aliyuncs.com/courses/1081/course-6-vaccine.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40417e-aed9-45f5-9bee-1a0100332c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the dataset and preview\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"course-6-vaccine.csv\", header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b20bc-b244-4b7c-8094-c8437843b81a",
   "metadata": {},
   "source": [
    "As you can see, the dataset consists of two columns where `Year` represents year and `Values` represents the corresponding world measles vaccination rate, and only the numerical portion of the percentage is taken here. We plot the data into charts to see the trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e5390-f4df-4452-afda-3b6a86927322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot\n",
    "\"\"\"\n",
    "# Define x and y\n",
    "x = df['Year']\n",
    "y = df['Values']\n",
    "# Plot\n",
    "plt.plot(x, y, 'r')\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c838ba-36d4-4fa3-8824-4fcec4c8db0f",
   "metadata": {},
   "source": [
    "For the trend shown in the above figure, we might think that polynomial regression is better than linear regression. Is this the case? Let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea238f25-f7cd-4d59-bb3e-776f8677c5a1",
   "metadata": {},
   "source": [
    "### Comparison: Linear Regression and Quadratic Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e98fd-fe89-402a-a834-adf3e759804e",
   "metadata": {},
   "source": [
    "According to what was learned in the linear regression course, in machine learning tasks we generally divide the dataset into training sets and test sets. So, here 70% of the data is considered the training sets and the rest 30% is classified as the test sets. Code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc8c55-ea6e-400e-9451-2c1258d246eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data partition\n",
    "\"\"\"\n",
    "# Divide dataframe into training set and testing set\n",
    "train_df = df[:int(len(df)*0.7)] \n",
    "test_df = df[int(len(df)*0.7):]\n",
    "\n",
    "# Define x and y for training and testing, respectively\n",
    "train_x = train_df['Year'].values\n",
    "train_y = train_df['Values'].values\n",
    "\n",
    "test_x = test_df['Year'].values\n",
    "test_y = test_df['Values'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdd3f7-3c23-491e-982b-7d5ad06d316e",
   "metadata": {},
   "source": [
    "Next, we use the polynomial regression prediction method provided by `scikit-learn` to train the model. First let's solve the above problem: **Is polynomial regression better than linear regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d985c9d-b71a-4cc5-8b89-c44ee6033b70",
   "metadata": {},
   "source": [
    "To begin with, we train the linear regression model and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec50cf0-3892-455c-a3a5-8fdd919049e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression predictions\n",
    "\"\"\"\n",
    "# Implement linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(train_x.reshape(len(train_x),1), train_y.reshape(len(train_y),1))\n",
    "results = model.predict(test_x.reshape(len(test_x),1))\n",
    "results # testing result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc75b33-207c-4e41-9f2c-e5bd9941d711",
   "metadata": {},
   "source": [
    "With the predictions, we can compare them to the ground truth. Here we use two indicators: mean absolute error (MAE) and mean squared error (MSE). If you are still not familiar with these two indicators, see their definitions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512ce32-43c5-4141-92a9-a2e230b83726",
   "metadata": {},
   "source": [
    "The **mean absolute error (MAE)** is the average of absolute errors. It can be calculated as follows:\n",
    "\\begin{equation}\n",
    "\\textrm{MAE}(y, \\hat{y} ) = \\frac{1}{n}\\sum_{i=1}^{n}{|y_{i}-\\hat y_{i}|}\\tag{6}\n",
    "\\end{equation}\n",
    "Where $y_{i}$ represents the true value, $\\hat y_{i}$ represents the predicted value and $n$ represents the number of values. Smaller the value of MAE, the better the accuracy of the predictive model. We can implement the MAE calculation function in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b05676-ab35-415f-a5eb-35ad77076cb5",
   "metadata": {},
   "source": [
    "**Mean Squared Error (MSE)** represents the expected value of the square of the error. Its calculation formula is as follows: \n",
    "\\begin{equation}\n",
    "\\textrm{MSE}(y, \\hat{y} ) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^{2}\\tag{7}\n",
    "\\end{equation}\n",
    "Where $y_{i}$ represents the true value, $\\hat y_{i}$ represents the predicted value and $n$ represents the number of values. Smaller the value of MSE, the better the accuracy of the predictive model. Also, we can implement the MAE calculation function in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd98cdf-81bd-4915-af03-1c40b18c2e22",
   "metadata": {},
   "source": [
    "Here, we use the MAE and MSE provided by `scikit-learn` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c43bdc-0b48-41b2-b28f-b5d26b6f88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Errors of linear regression\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MAE of linear regression: \", mean_absolute_error(test_y, results.flatten()))\n",
    "print(\"MSE of linear regression: \", mean_squared_error(test_y, results.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72624a-2ab1-4f44-8909-54b9f99c5f6a",
   "metadata": {},
   "source": [
    "Next, we can start training the quadratic polynomial regression model and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55214641-f636-4bdf-ba75-178480b85b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Quadratic polynomial predictions\n",
    "\"\"\"\n",
    "# Generate the feature matrix\n",
    "poly_features_2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_train_x_2 = poly_features_2.fit_transform(train_x.reshape(len(train_x),1))\n",
    "poly_test_x_2 = poly_features_2.fit_transform(test_x.reshape(len(test_x),1))\n",
    "\n",
    "# Training and predicting\n",
    "model = LinearRegression()\n",
    "model.fit(poly_train_x_2, train_y.reshape(len(train_x),1)) # Train the model\n",
    "\n",
    "results_2 = model.predict(poly_test_x_2) # Prediction\n",
    "\n",
    "results_2.flatten() # Print after flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da1a06-aa6b-4937-9fa9-3370b88a4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Errors of quadratic polynomial regression\n",
    "\"\"\"\n",
    "print(\"MAE of quadratic polynomial regression: \", mean_absolute_error(test_y, results_2.flatten()))\n",
    "print(\"MSE of quadratic polynomial regression: \", mean_squared_error(test_y, results_2.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c77001-c7bc-430e-840d-e28202609baa",
   "metadata": {},
   "source": [
    "Based on the above definitions of MAE and MSE, you now know that smaller the two values, the higher the prediction accuracy of the model. In other words, the prediction results of the linear regression model are better than those of the quadratic polynomial regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da8303-7453-4c76-8ded-2612ad67dc3d",
   "metadata": {},
   "source": [
    "### Polynomial Regression Predictions with Higher Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e2d8f-7259-4f57-b0e5-59e3bf8333d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actually, this situation is very common. But this does not mean that the polynomial regression will always be worse than linear regression. Next let's try the results of the `3-`, `4-` and `5-` `degree` polynomial regressions. To simplify the process, we reconstructed the code and got the predicted results for three simulations at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd5533-7e61-4674-9fb9-493b93a5d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "By instantiating the `make_pipeline` pipeline class, all predictors can call `fit` and `predict` instructions. `make_pipeline` is a technique innovation of `sklearn`, and no more details will be introduced here. We just imitate the official [tutorials](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) to simple use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af6789-089b-408e-a1c7-8793468a9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Polynomial regression predictions with higher degrees\n",
    "\"\"\"\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "train_x = train_x.reshape(len(train_x),1)\n",
    "test_x = test_x.reshape(len(test_x),1)\n",
    "train_y = train_y.reshape(len(train_y),1)\n",
    "\n",
    "for m in [3, 4, 5]:\n",
    "    model = make_pipeline(PolynomialFeatures(m, include_bias=False), LinearRegression())\n",
    "    model.fit(train_x, train_y)\n",
    "    pre_y = model.predict(test_x)\n",
    "    print(\"{}-degree polynomial regression_MAE: \".format(m), mean_absolute_error(test_y, pre_y.flatten()))\n",
    "    print(\"{}-degree polynomial regression_MSE: \".format(m), mean_squared_error(test_y, pre_y.flatten()))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8f2bc-9d03-4bc4-affc-8c8b06cdd8e0",
   "metadata": {},
   "source": [
    "From the above results, it can be concluded that the results of the `3-`, `4-` and `5-` `degree` polynomial regressions are better than those of the linear regression model. Therefore, polynomial regression still has its superiority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e499758-f0df-45de-b11f-6df4cc98f470",
   "metadata": {},
   "source": [
    "### The Optimal Degree in Polynomial Regression Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d325a82-bfb8-42c3-9a20-1c2e2cb43a5c",
   "metadata": {},
   "source": [
    "Up to now, you may have a question: In the process of selecting a polynomial for regression prediction, **what degree is optimal?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5100f6e-b354-4e2a-b486-0c8fff6dfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "For the above question, the answer is simple. We can choose an error indicator. For example, if you select MSE here, then we can calculate the errors with different degrees. Just have a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058888cb-7fba-460a-8cd8-e9ece473d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate MSE results of m-degree polynomial regression and plot\n",
    "\"\"\"\n",
    "mse = [] # Save MSE for different degrees\n",
    "m = 1 # Start from 1-degree\n",
    "m_max = 10 # Set the highest degree to be tested\n",
    "while m <= m_max:\n",
    "    model = make_pipeline(PolynomialFeatures(m, include_bias=False), LinearRegression())\n",
    "    model.fit(train_x, train_y) # Train\n",
    "    pre_y = model.predict(test_x) # Test\n",
    "    mse.append(mean_squared_error(test_y, pre_y.flatten())) # Calculate MSE\n",
    "    m = m + 1\n",
    "\n",
    "print(\"MSE results: \", mse)\n",
    "# Plot\n",
    "plt.plot([i for i in range(1, m_max + 1)], mse, 'r')\n",
    "plt.scatter([i for i in range(1, m_max + 1)], mse)\n",
    "\n",
    "# Descriptions\n",
    "plt.title(\"MSE of m degree of polynomial regression\")\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f8b32-e1ae-47af-a996-35ae5393f822",
   "metadata": {},
   "source": [
    "As shown in the figure above, the MSE value reached its highest point in the 2nd degree polynomial regression prediction and then decreased rapidly. The results after the 3rd degree still showed a gradual decline, but they tended to be stable. In general, we also need to consider the generalization ability of the model and avoid overfitting. Here, we can choose the 3rd degree polynomial as the optimal regression prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c4f69-f08c-4198-9320-2877941c9d75",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f1599-e2a9-4556-830c-39b735bee5c2",
   "metadata": {},
   "source": [
    "In this experiment, we learned what polynomial regression is, and the connections and differences between polynomial regression and linear regression. At the same time, the experiment explores the implementation of polynomial regression fitting and uses `scikit-learn` to construct a polynomial regression prediction model under a real dataset. The knowledge points involved in the experiment are:\n",
    "- Polynomial\n",
    "- Polynomial fitting\n",
    "- Least squares method\n",
    "- Overfitting\n",
    "- Dataset partition\n",
    "- Selection of optimal model\n",
    "- `scikit-learn` implementations of polynomial regression prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
