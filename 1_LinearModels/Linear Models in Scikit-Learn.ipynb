{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746ae7b3-1e29-41db-ac9a-52442b1fb4e7",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) is a linear regression method that minimizes the sum of squared differences between the observed targets and the predicted targets. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db106c0-39be-4fb3-a3a3-6d6c20a1ce12",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) å›æ­¸ä»‹ç´¹\n",
    "\n",
    "**Ordinary Least Squares (OLS)** æ˜¯ç·šæ€§å›æ­¸ä¸­æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œç›®çš„æ˜¯æ‰¾å‡ºä¸€æ¢æœ€ä½³ç›´ç·šï¼Œä½¿é æ¸¬å€¼èˆ‡å¯¦éš›å€¼ä¹‹é–“çš„**èª¤å·®å¹³æ–¹å’Œï¼ˆSum of Squared Errors, SSEï¼‰æœ€å°**ã€‚\n",
    "\n",
    "---\n",
    "ç·šæ€§æ¨¡å‹å‡è¨­ç‚ºï¼š\n",
    "$$y = X\\beta + \\varepsilon$$\n",
    "\n",
    "çµ¦å®šè³‡æ–™é›†ï¼š\n",
    "<ul>\n",
    "  <li>\\( y \\in \\mathbb{R}^n \\)ï¼šç›®æ¨™è®Šæ•¸ï¼ˆresponse vectorï¼‰</li>\n",
    "  <li>\\( X \\in \\mathbb{R}^{n \\times p} \\)ï¼šè§£é‡‹è®Šæ•¸çŸ©é™£ï¼ˆdesign matrixï¼‰</li>\n",
    "  <li>\\( \\beta \\in \\mathbb{R}^p \\)ï¼šæ¨¡å‹åƒæ•¸</li>\n",
    "  <li>\\( \\varepsilon \\in \\mathbb{R}^n \\)ï¼šèª¤å·®é …</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "æœ€ä½³åŒ–ç›®æ¨™ï¼ŒOLS è¦æœ€å°åŒ–çš„æå¤±å‡½æ•¸ï¼ˆèª¤å·®å¹³æ–¹å’Œï¼‰ç‚ºï¼š$ \\min_{\\beta} \\; \\| y - X\\beta \\|^2 $\n",
    "\n",
    "---\n",
    "\n",
    "å°å‡ºè§£ï¼ˆClosed-form solutionï¼‰ï¼Œå°‡æå¤±å‡½æ•¸å° $\\beta$ å¾®åˆ†ä¸¦è¨­ç‚º 0ï¼Œå¾—åˆ° OLS çš„å°é–‰è§£ç‚ºï¼š$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $\n",
    "\n",
    "> **æ¢ä»¶**ï¼š$X^\\top X$ å¿…é ˆç‚ºå¯é€†çŸ©é™£ï¼ˆé€šå¸¸éœ€æ»¿è¶³ X ç„¡å®Œå…¨å…±ç·šæ€§ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "OLS å»£æ³›æ‡‰ç”¨æ–¼ï¼š\n",
    "- é æ¸¬èˆ‡è§£é‡‹è®Šæ•¸é–“ç·šæ€§é—œä¿‚\n",
    "- ç‰¹å¾µé¸æ“‡èˆ‡åˆæ­¥å»ºæ¨¡\n",
    "- ç¶“æ¿Ÿå­¸ã€ç¤¾æœƒç§‘å­¸ã€æ©Ÿå™¨å­¸ç¿’ä¸­çš„åŸºç¤æ¨¡å‹\n",
    "\n",
    "---\n",
    "\n",
    "### åƒè€ƒ\n",
    "\n",
    "- Hastie, Tibshirani, and Friedman, *The Elements of Statistical Learning*\n",
    "- ISLR, *An Introduction to Statistical Learning*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e8471-e7ba-4b8e-aae6-1448f555c238",
   "metadata": {},
   "source": [
    "#### Let's start by fitting a linear regression model using OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf6d2a6-4108-4a02-bd2e-2630740c6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b905148-81c2-4426-b620-f5e6fb879633",
   "metadata": {},
   "source": [
    "*  We import the linear_model module from scikit-learn.\n",
    "*  We create an instance of LinearRegression.\n",
    "*  We use the fit method to fit the model to the training data.\n",
    "*  We print the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aeb769-8105-48ad-92f3-cdeaff632947",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75cd85-b5ca-4e4e-ab97-c21d2b0b3103",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge regression is a linear regression method that adds a penalty term to the ordinary least squares objective function. This penalty term helps to reduce overfitting by shrinking the coefficients towards zero. The complexity of the model can be controlled by the regularization parameter.\n",
    "\n",
    "Let's fit a ridge regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d6328-5a75-4074-b85b-e7cf54cba202",
   "metadata": {},
   "source": [
    "# Ridge Regressionï¼ˆè„Šå›æ­¸ï¼‰ä»‹ç´¹\n",
    "\n",
    "**Ridge Regression** æ˜¯ä¸€ç¨®æ”¹é€²çš„ç·šæ€§å›æ­¸æ–¹æ³•ï¼Œåœ¨ Ordinary Least Squares (OLS) çš„ç›®æ¨™å‡½æ•¸ä¸­åŠ å…¥æ‡²ç½°é …ï¼ˆpenalty termï¼‰ï¼Œç”¨ä¾†æŠ‘åˆ¶æ¨¡å‹éåº¦æ“¬åˆï¼ˆoverfittingï¼‰ã€‚å®ƒé€éå°‡æ¨¡å‹ä¿‚æ•¸å‘ 0 æ”¶ç¸®ï¼Œé”åˆ°ç°¡åŒ–æ¨¡å‹èˆ‡æé«˜æ³›åŒ–èƒ½åŠ›çš„ç›®çš„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ç›®æ¨™å‡½æ•¸ï¼ˆObjective Functionï¼‰\n",
    "\n",
    "Ridge Regression çš„æœ€å°åŒ–ç›®æ¨™ç‚ºï¼š\n",
    "\n",
    "$$\\min_{\\beta} \\; \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|^2$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $y$ æ˜¯éŸ¿æ‡‰è®Šæ•¸ï¼ˆresponse variableï¼‰\n",
    "- $X$ æ˜¯ç‰¹å¾µçŸ©é™£ï¼ˆfeature matrixï¼‰\n",
    "- $\\beta$ æ˜¯æ¨¡å‹åƒæ•¸ï¼ˆregression coefficientsï¼‰\n",
    "- $\\lambda$ æ˜¯æ­£å‰‡åŒ–åƒæ•¸ï¼ˆregularization parameterï¼‰ï¼Œæ§åˆ¶æ‡²ç½°é …çš„å¼·åº¦\n",
    "- $\\| \\beta \\|^2$ æ˜¯ L2 ç¯„æ•¸å¹³æ–¹ï¼ˆå³ $\\sum \\beta_i^2$ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## æ¨¡å‹ç‰¹æ€§\n",
    "\n",
    "- ç•¶ $\\lambda = 0$ï¼ŒRidge Regression ç­‰åŒæ–¼ OLSã€‚\n",
    "- éš¨è‘— $\\lambda$ å¢åŠ ï¼Œæ¨¡å‹ä¿‚æ•¸æœƒè¶Šæ¥è¿‘ 0ã€‚\n",
    "- èƒ½æœ‰æ•ˆè™•ç†å…±ç·šæ€§å•é¡Œï¼Œä¸¦é˜²æ­¢æ¨¡å‹å°è¨“ç·´è³‡æ–™éåº¦æ“¬åˆã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## èˆ‡ OLS çš„æ¯”è¼ƒ\n",
    "\n",
    "| é …ç›®         | OLS å›æ­¸           | Ridge å›æ­¸                      |\n",
    "|--------------|--------------------|---------------------------------|\n",
    "| ç›®æ¨™         | æœ€å°åŒ–æ®˜å·®å¹³æ–¹å’Œ    | æ®˜å·®å¹³æ–¹å’Œ + æ­£å‰‡åŒ–é …          |\n",
    "| éæ“¬åˆæ§åˆ¶    | ç„¡                  | æœ‰ï¼Œé€é $\\lambda$ æ§åˆ¶        |\n",
    "| è§£çš„å½¢å¼      | $(X^\\top X)^{-1} X^\\top y$ | $(X^\\top X + \\lambda I)^{-1} X^\\top y$ |\n",
    "\n",
    "---\n",
    "\n",
    "## æ‡‰ç”¨å ´æ™¯\n",
    "\n",
    "- é«˜ç¶­è³‡æ–™ä¸­ï¼ˆç‰¹å¾µæ•¸å¤šæ–¼æ¨£æœ¬æ•¸ï¼‰\n",
    "- ç‰¹å¾µä¹‹é–“æœ‰é«˜åº¦å…±ç·šæ€§\n",
    "- å¸Œæœ›æ¨¡å‹æ›´ç©©å®šä¸¦å…·å‚™æ›´å¼·çš„æ³›åŒ–èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28820801-e94f-4c88-8f66-a3215968cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=0.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, 0.1, 1])\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aab900-6f34-40a7-998d-fcd05c9519bc",
   "metadata": {},
   "source": [
    "* We create an instance of Ridge with the regularization parameter alpha set to 0.5.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the ridge regression model.\n",
    "* Continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40039ebb-2e35-473d-a468-a138e80e9d46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fcd6c-b2eb-4b6e-936e-5000f0fdbca2",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "Lasso is a linear regression method that adds a penalty term to the ordinary least squares objective function. The penalty term has the effect of setting some coefficients to exactly zero, thus performing feature selection. Lasso can be used for sparse model estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c7e83-06b6-4350-a314-4c0aab0d1096",
   "metadata": {},
   "source": [
    "# Lasso Regressionï¼ˆå¥—ç´¢å›æ­¸ï¼‰ä»‹ç´¹\n",
    "\n",
    "**Lasso Regression** æ˜¯ä¸€ç¨®ç·šæ€§å›æ­¸æ–¹æ³•ï¼Œå®ƒåœ¨ Ordinary Least Squares (OLS) çš„ç›®æ¨™å‡½æ•¸ä¸­åŠ å…¥ L1 æ­£å‰‡åŒ–ï¼ˆL1 penaltyï¼‰ã€‚é€™ç¨®æ‡²ç½°æœƒå°è‡´éƒ¨åˆ†ä¿‚æ•¸ç¸®å°è‡³ã€Œ**å®Œå…¨ç‚º 0**ã€ï¼Œå› æ­¤å…·æœ‰ **ç‰¹å¾µé¸æ“‡ï¼ˆfeature selectionï¼‰** çš„èƒ½åŠ›ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ç›®æ¨™å‡½æ•¸ï¼ˆObjective Functionï¼‰\n",
    "\n",
    "Lasso çš„å„ªåŒ–ç›®æ¨™ç‚ºï¼š\n",
    "\n",
    "$$ \\min_{\\beta} \\; \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_1 $$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $y$ï¼šéŸ¿æ‡‰è®Šæ•¸ï¼ˆresponse variableï¼‰\n",
    "- $X$ï¼šç‰¹å¾µçŸ©é™£ï¼ˆfeature matrixï¼‰\n",
    "- $\\beta$ï¼šå›æ­¸ä¿‚æ•¸ï¼ˆregression coefficientsï¼‰\n",
    "- $\\lambda$ï¼šæ­£å‰‡åŒ–åƒæ•¸ï¼ˆregularization parameterï¼‰\n",
    "- $\\| \\beta \\|_1 = \\sum |\\beta_i|$ï¼šL1 ç¯„æ•¸ï¼ˆçµ•å°å€¼ä¹‹å’Œï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ç‰¹æ€§èˆ‡å„ªé»\n",
    "\n",
    "- å¯ä»¥å°‡ä¸é‡è¦çš„ç‰¹å¾µä¿‚æ•¸ç¸®ç‚º **0**ï¼Œé”åˆ°è®Šæ•¸é¸æ“‡æ•ˆæœ\n",
    "- åœ¨ç‰¹å¾µæ•¸é‡é å¤§æ–¼æ¨£æœ¬æ•¸çš„æƒ…æ³ä¸‹éå¸¸æœ‰æ•ˆ\n",
    "- é©åˆå»ºç«‹ç¨€ç–æ¨¡å‹ï¼ˆsparse modelï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## èˆ‡ Ridge Regression æ¯”è¼ƒ\n",
    "\n",
    "| é …ç›®         | Ridge å›æ­¸         | Lasso å›æ­¸                     |\n",
    "|--------------|--------------------|--------------------------------|\n",
    "| æ‡²ç½°é¡å‹      | L2 ç¯„æ•¸             | L1 ç¯„æ•¸                         |\n",
    "| ä¿‚æ•¸ç¸®å°      | ä¿‚æ•¸è¶¨è¿‘æ–¼ 0         | ä¿‚æ•¸å¯ä»¥è®Šç‚º 0ï¼ˆç‰¹å¾µé¸æ“‡ï¼‰      |\n",
    "| é©ç”¨æƒ…å¢ƒ      | å…±ç·šæ€§è™•ç†          | ç¨€ç–ç‰¹å¾µå»ºæ¨¡ï¼ç‰¹å¾µé¸æ“‡          |\n",
    "\n",
    "---\n",
    "\n",
    "## æº–å‚™å»ºç«‹ Lasso æ¨¡å‹\n",
    "\n",
    "æˆ‘å€‘æ¥ä¸‹ä¾†å¯ä»¥å˜—è©¦ä½¿ç”¨ Python å»ºç«‹ä¸€å€‹ Lasso å›æ­¸æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e450640-d698-4440-b1fb-1ddd4425bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.45386948 94.67737779 40.55794675  0.         -0.         -0.\n",
      " 10.98839003 95.40225582 80.67610577 34.83198456 -0.          0.\n",
      "  0.         -0.         -0.         29.82848069  7.12581798 -0.\n",
      "  0.         52.17111821]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# å»ºç«‹æ¨¡æ“¬è³‡æ–™\n",
    "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# å»ºç«‹ä¸¦è¨“ç·´ Lasso æ¨¡å‹\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# å°å‡ºæ¨¡å‹ä¿‚æ•¸\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa9738e-bae7-43ed-9969-89db71a6c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0. ]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf50eb3-de15-4e04-ab5a-34bb36602e50",
   "metadata": {},
   "source": [
    "* We create an instance of Lasso with the regularization parameter alpha set to 0.1.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e386-dfc3-486c-b467-a14426ef7659",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef2429-0fd4-449f-819a-951eb92c6a69",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression is a classification method that estimates the probabilities of the possible outcomes using a logistic function. It is commonly used for binary classification tasks. Logistic regression can also be extended to handle multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15915-b221-418e-8a9c-583d2e53bde9",
   "metadata": {},
   "source": [
    "# Logistic Regressionï¼ˆé‚è¼¯å›æ­¸ï¼‰ä»‹ç´¹\n",
    "\n",
    "**Logistic Regression** æ˜¯ä¸€ç¨®ç”¨æ–¼åˆ†é¡ä»»å‹™çš„çµ±è¨ˆæ–¹æ³•ï¼Œé€é logistic å‡½æ•¸ï¼ˆS å‹å‡½æ•¸ï¼‰å°‡ç·šæ€§å›æ­¸è¼¸å‡ºè½‰æ›ç‚ºä»‹æ–¼ 0 èˆ‡ 1 ä¹‹é–“çš„æ©Ÿç‡å€¼ã€‚å®ƒæœ€å¸¸ç”¨æ–¼ **äºŒå…ƒåˆ†é¡ï¼ˆbinary classificationï¼‰** å•é¡Œï¼Œä¾‹å¦‚åˆ¤æ–·æ˜¯å¦æœƒç™¼ç”ŸæŸäº‹ä»¶ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## æ¨¡å‹æ¦‚å¿µ\n",
    "\n",
    "Logistic å›æ­¸å»ºæ¨¡çš„æ˜¯äº‹ä»¶ç™¼ç”Ÿçš„ **æ©Ÿç‡**ï¼Œæ¨¡å‹å½¢å¼ç‚ºï¼š\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid X) = \\frac{1}{1 + e^{-(X\\beta)}}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $P(y = 1 \\mid X)$ æ˜¯åœ¨çµ¦å®šè¼¸å…¥ $X$ æ¢ä»¶ä¸‹ï¼Œ$y$ ç‚º 1 çš„æ©Ÿç‡  \n",
    "- $X$ æ˜¯ç‰¹å¾µå‘é‡ï¼Œ$\\beta$ æ˜¯åƒæ•¸å‘é‡  \n",
    "- $e$ æ˜¯è‡ªç„¶å°æ•¸çš„åº•æ•¸  \n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ ç‚º sigmoid å‡½æ•¸\n",
    "\n",
    "---\n",
    "\n",
    "## ç‰¹æ€§èˆ‡æ“´å……\n",
    "\n",
    "- æ¨¡å‹è¼¸å‡ºç‚ºæ©Ÿç‡ï¼Œå¯æ ¹æ“šé–¾å€¼ï¼ˆé€šå¸¸ç‚º 0.5ï¼‰è½‰æ›ç‚ºé¡åˆ¥é æ¸¬\n",
    "- å¯å»¶ä¼¸ç‚ºå¤šåˆ†é¡æ¨¡å‹ï¼ˆmultinomial logistic regression / softmax regressionï¼‰\n",
    "- å¯åŠ å…¥ L1 æˆ– L2 æ­£å‰‡åŒ–é …é€²è¡Œæ¨¡å‹æ§åˆ¶\n",
    "\n",
    "---\n",
    "\n",
    "## èˆ‡ç·šæ€§å›æ­¸çš„å·®ç•°\n",
    "\n",
    "| é …ç›®             | ç·šæ€§å›æ­¸               | é‚è¼¯å›æ­¸                 |\n",
    "|------------------|------------------------|----------------------------|\n",
    "| è¼¸å‡ºç¯„åœ         | ä»»æ„å¯¦æ•¸               | æ©Ÿç‡å€¼ï¼ˆ0 åˆ° 1ï¼‰            |\n",
    "| å•é¡Œé¡å‹         | é æ¸¬é€£çºŒå€¼             | åˆ†é¡å•é¡Œï¼ˆæ©Ÿç‡/é¡åˆ¥ï¼‰       |\n",
    "| æå¤±å‡½æ•¸         | æ®˜å·®å¹³æ–¹å’Œï¼ˆMSEï¼‰       | é‚è¼¯æå¤±ï¼ˆlog loss / cross entropyï¼‰ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e8d575-b044-4ce5-a995-cbbe78f5bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# å»ºç«‹æ¨¡æ“¬è³‡æ–™ï¼ˆbinary classificationï¼‰\n",
    "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
    "\n",
    "# åˆ†å‰²è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# å»ºç«‹ä¸¦è¨“ç·´æ¨¡å‹\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19de345b-344d-46c7-a1b7-d7d3af0ebcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39758121 -0.50380348  0.07252856  1.87977126  0.08573005]]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(random_state=0).fit(X, y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e1e3b-025c-4d80-abf5-877023138b80",
   "metadata": {},
   "source": [
    "* We create an instance of LogisticRegression with the random_state parameter set to 0.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768d3d3-03f4-48f4-a867-ca53bb76a0f5",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is a simple yet efficient approach for training linear models. It is particularly useful when the number of samples and features is very large. SGD updates the model parameters using a small subset of the training data at each iteration, which makes it suitable for online learning and out-of-core learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8bd86-eedf-4fff-bbb7-77414f594f03",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) ä»‹ç´¹\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** æ˜¯ä¸€ç¨®ç°¡å–®è€Œé«˜æ•ˆçš„æ¨¡å‹è¨“ç·´æ–¹æ³•ï¼Œç‰¹åˆ¥é©ç”¨æ–¼æ¨£æœ¬æ•¸æˆ–ç‰¹å¾µæ•¸éå¸¸é¾å¤§çš„æƒ…æ³ã€‚èˆ‡å‚³çµ±çš„æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™ï¼ˆBatch Gradient Descentï¼‰ä¸åŒï¼ŒSGD æ¯æ¬¡åƒ…ä½¿ç”¨ä¸€ç­†æˆ–å°‘é‡è³‡æ–™ä¾†æ›´æ–°åƒæ•¸ï¼Œé€™ä½¿å¾—å®ƒç‰¹åˆ¥é©åˆï¼š\n",
    "\n",
    "- ç·šä¸Šå­¸ç¿’ï¼ˆonline learningï¼‰\n",
    "- æµå¼è³‡æ–™è™•ç†ï¼ˆstreaming dataï¼‰\n",
    "- è¶…å¤§è³‡æ–™é›†ï¼ˆout-of-core learningï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## åŸºæœ¬æ¦‚å¿µ\n",
    "\n",
    "å°æ–¼ä¸€å€‹åƒæ•¸ç‚º $\\theta$ çš„æ¨¡å‹ï¼Œè‹¥æå¤±å‡½æ•¸ç‚º $L(\\theta)$ï¼Œå‰‡å‚³çµ±æ¢¯åº¦ä¸‹é™çš„æ›´æ–°å…¬å¼ç‚ºï¼š\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "è€Œ **SGD** å‰‡æ˜¯ä½¿ç”¨éš¨æ©Ÿé¸å–çš„å–®ç­†è³‡æ–™ $(x_i, y_i)$ ä¾†è¿‘ä¼¼æå¤±ï¼š\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla L_i(\\theta)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $\\eta$ æ˜¯å­¸ç¿’ç‡ï¼ˆlearning rateï¼‰\n",
    "- $L_i(\\theta)$ æ˜¯å°æ¨£æœ¬ $i$ çš„æå¤±\n",
    "\n",
    "é€™æ¨£çš„æ›´æ–°é€Ÿåº¦å¿«ã€è¨˜æ†¶é«”éœ€æ±‚ä½ï¼Œä½†æœƒå¸¶ä¾†è¼ƒé«˜çš„åƒæ•¸éœ‡ç›ªï¼ˆä¸ç©©å®šæ€§ï¼‰ï¼Œé€šå¸¸å¯é€é mini-batchã€å‹•æ…‹å­¸ç¿’ç‡æˆ–å‹•é‡æ³•æ”¹å–„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## å„ªé»èˆ‡ç‰¹æ€§\n",
    "\n",
    "- âš¡ **å¿«é€Ÿæ›´æ–°**ï¼šé©åˆå³æ™‚å­¸ç¿’èˆ‡æ¨¡å‹å¾®èª¿\n",
    "- ğŸ“‰ **ä½è¨˜æ†¶é«”ä½¿ç”¨**ï¼šæ¯æ¬¡åªè¼‰å…¥å°‘é‡è³‡æ–™\n",
    "- ğŸ”„ **å¯æŒçºŒè¨“ç·´**ï¼šæ”¯æ´ streaming / online learning æ¨¡å¼\n",
    "- âš ï¸ **æ”¶æ–‚è¼ƒä¸ç©©å®š**ï¼šéœ€èª¿æ•´å­¸ç¿’ç‡èˆ‡å…¶ä»–è¶…åƒæ•¸\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1fa89-1219-4eb5-9b3b-3868d61f839b",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ SGD è¨“ç·´ Logistic Regression æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8d1bd9-dcba-4de6-8a6b-696cb2016bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# å»ºç«‹æ¨¡æ“¬è³‡æ–™\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# åˆ‡åˆ†è³‡æ–™\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# å»ºç«‹ä¸¦è¨“ç·´ SGD åˆ†é¡å™¨ï¼ˆä½¿ç”¨é‚è¼¯æ–¯å›æ­¸æå¤±ï¼‰\n",
    "model = SGDClassifier(loss=\"log_loss\", max_iter=1000, learning_rate='optimal', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9300e5d5-0e91-4b02-b42c-bd401a73d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11068527 -0.86440343  0.71723043 -0.8459125  -0.20550116  2.76668295\n",
      "   0.04900348 -0.39310887 -0.60430234 -0.26534078  0.5088726   0.70344231\n",
      "   0.0689058  -0.01342635 -0.8484677   0.52268594 -0.8268332  -0.49510664\n",
      "  -1.3159977  -1.01810651]]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(loss=\"log_loss\", max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568702b-4079-40a4-9473-8ed1abbacf68",
   "metadata": {},
   "source": [
    "* We create an instance of SGDClassifier with the loss parameter set to \"log_loss\" to perform logistic regression.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the logistic regression model obtained using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c2dc5-c8d4-4943-b1f7-7cb8d543e8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
