{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746ae7b3-1e29-41db-ac9a-52442b1fb4e7",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) is a linear regression method that minimizes the sum of squared differences between the observed targets and the predicted targets. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db106c0-39be-4fb3-a3a3-6d6c20a1ce12",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) 回歸介紹\n",
    "\n",
    "**Ordinary Least Squares (OLS)** 是線性回歸中最常用的方法，目的是找出一條最佳直線，使預測值與實際值之間的**誤差平方和（Sum of Squared Errors, SSE）最小**。\n",
    "\n",
    "---\n",
    "線性模型假設為：\n",
    "$$y = X\\beta + \\varepsilon$$\n",
    "\n",
    "給定資料集：\n",
    "<ul>\n",
    "  <li>\\( y \\in \\mathbb{R}^n \\)：目標變數（response vector）</li>\n",
    "  <li>\\( X \\in \\mathbb{R}^{n \\times p} \\)：解釋變數矩陣（design matrix）</li>\n",
    "  <li>\\( \\beta \\in \\mathbb{R}^p \\)：模型參數</li>\n",
    "  <li>\\( \\varepsilon \\in \\mathbb{R}^n \\)：誤差項</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "最佳化目標，OLS 要最小化的損失函數（誤差平方和）為：$ \\min_{\\beta} \\; \\| y - X\\beta \\|^2 $\n",
    "\n",
    "---\n",
    "\n",
    "導出解（Closed-form solution），將損失函數對 $\\beta$ 微分並設為 0，得到 OLS 的封閉解為：$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $\n",
    "\n",
    "> **條件**：$X^\\top X$ 必須為可逆矩陣（通常需滿足 X 無完全共線性）\n",
    "\n",
    "---\n",
    "\n",
    "### 應用場景\n",
    "\n",
    "OLS 廣泛應用於：\n",
    "- 預測與解釋變數間線性關係\n",
    "- 特徵選擇與初步建模\n",
    "- 經濟學、社會科學、機器學習中的基礎模型\n",
    "\n",
    "---\n",
    "\n",
    "### 參考\n",
    "\n",
    "- Hastie, Tibshirani, and Friedman, *The Elements of Statistical Learning*\n",
    "- ISLR, *An Introduction to Statistical Learning*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e8471-e7ba-4b8e-aae6-1448f555c238",
   "metadata": {},
   "source": [
    "#### Let's start by fitting a linear regression model using OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf6d2a6-4108-4a02-bd2e-2630740c6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "X = [[0, 0], [1, 1], [2, 2]]\n",
    "y = [0, 1, 2]\n",
    "reg.fit(X, y)\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b905148-81c2-4426-b620-f5e6fb879633",
   "metadata": {},
   "source": [
    "*  We import the linear_model module from scikit-learn.\n",
    "*  We create an instance of LinearRegression.\n",
    "*  We use the fit method to fit the model to the training data.\n",
    "*  We print the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aeb769-8105-48ad-92f3-cdeaff632947",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75cd85-b5ca-4e4e-ab97-c21d2b0b3103",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Ridge regression is a linear regression method that adds a penalty term to the ordinary least squares objective function. This penalty term helps to reduce overfitting by shrinking the coefficients towards zero. The complexity of the model can be controlled by the regularization parameter.\n",
    "\n",
    "Let's fit a ridge regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d6328-5a75-4074-b85b-e7cf54cba202",
   "metadata": {},
   "source": [
    "# Ridge Regression（脊回歸）介紹\n",
    "\n",
    "**Ridge Regression** 是一種改進的線性回歸方法，在 Ordinary Least Squares (OLS) 的目標函數中加入懲罰項（penalty term），用來抑制模型過度擬合（overfitting）。它透過將模型係數向 0 收縮，達到簡化模型與提高泛化能力的目的。\n",
    "\n",
    "---\n",
    "\n",
    "## 目標函數（Objective Function）\n",
    "\n",
    "Ridge Regression 的最小化目標為：\n",
    "\n",
    "$$\\min_{\\beta} \\; \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|^2$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $y$ 是響應變數（response variable）\n",
    "- $X$ 是特徵矩陣（feature matrix）\n",
    "- $\\beta$ 是模型參數（regression coefficients）\n",
    "- $\\lambda$ 是正則化參數（regularization parameter），控制懲罰項的強度\n",
    "- $\\| \\beta \\|^2$ 是 L2 範數平方（即 $\\sum \\beta_i^2$）\n",
    "\n",
    "---\n",
    "\n",
    "## 模型特性\n",
    "\n",
    "- 當 $\\lambda = 0$，Ridge Regression 等同於 OLS。\n",
    "- 隨著 $\\lambda$ 增加，模型係數會越接近 0。\n",
    "- 能有效處理共線性問題，並防止模型對訓練資料過度擬合。\n",
    "\n",
    "---\n",
    "\n",
    "## 與 OLS 的比較\n",
    "\n",
    "| 項目         | OLS 回歸           | Ridge 回歸                      |\n",
    "|--------------|--------------------|---------------------------------|\n",
    "| 目標         | 最小化殘差平方和    | 殘差平方和 + 正則化項          |\n",
    "| 過擬合控制    | 無                  | 有，透過 $\\lambda$ 控制        |\n",
    "| 解的形式      | $(X^\\top X)^{-1} X^\\top y$ | $(X^\\top X + \\lambda I)^{-1} X^\\top y$ |\n",
    "\n",
    "---\n",
    "\n",
    "## 應用場景\n",
    "\n",
    "- 高維資料中（特徵數多於樣本數）\n",
    "- 特徵之間有高度共線性\n",
    "- 希望模型更穩定並具備更強的泛化能力\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28820801-e94f-4c88-8f66-a3215968cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34545455 0.34545455]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=0.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, 0.1, 1])\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aab900-6f34-40a7-998d-fcd05c9519bc",
   "metadata": {},
   "source": [
    "* We create an instance of Ridge with the regularization parameter alpha set to 0.5.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the ridge regression model.\n",
    "* Continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40039ebb-2e35-473d-a468-a138e80e9d46",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396fcd6c-b2eb-4b6e-936e-5000f0fdbca2",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "Lasso is a linear regression method that adds a penalty term to the ordinary least squares objective function. The penalty term has the effect of setting some coefficients to exactly zero, thus performing feature selection. Lasso can be used for sparse model estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c7e83-06b6-4350-a314-4c0aab0d1096",
   "metadata": {},
   "source": [
    "# Lasso Regression（套索回歸）介紹\n",
    "\n",
    "**Lasso Regression** 是一種線性回歸方法，它在 Ordinary Least Squares (OLS) 的目標函數中加入 L1 正則化（L1 penalty）。這種懲罰會導致部分係數縮小至「**完全為 0**」，因此具有 **特徵選擇（feature selection）** 的能力。\n",
    "\n",
    "---\n",
    "\n",
    "## 目標函數（Objective Function）\n",
    "\n",
    "Lasso 的優化目標為：\n",
    "\n",
    "$$ \\min_{\\beta} \\; \\| y - X\\beta \\|^2 + \\lambda \\| \\beta \\|_1 $$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $y$：響應變數（response variable）\n",
    "- $X$：特徵矩陣（feature matrix）\n",
    "- $\\beta$：回歸係數（regression coefficients）\n",
    "- $\\lambda$：正則化參數（regularization parameter）\n",
    "- $\\| \\beta \\|_1 = \\sum |\\beta_i|$：L1 範數（絕對值之和）\n",
    "\n",
    "---\n",
    "\n",
    "## 特性與優點\n",
    "\n",
    "- 可以將不重要的特徵係數縮為 **0**，達到變數選擇效果\n",
    "- 在特徵數量遠大於樣本數的情況下非常有效\n",
    "- 適合建立稀疏模型（sparse model）\n",
    "\n",
    "---\n",
    "\n",
    "## 與 Ridge Regression 比較\n",
    "\n",
    "| 項目         | Ridge 回歸         | Lasso 回歸                     |\n",
    "|--------------|--------------------|--------------------------------|\n",
    "| 懲罰類型      | L2 範數             | L1 範數                         |\n",
    "| 係數縮小      | 係數趨近於 0         | 係數可以變為 0（特徵選擇）      |\n",
    "| 適用情境      | 共線性處理          | 稀疏特徵建模／特徵選擇          |\n",
    "\n",
    "---\n",
    "\n",
    "## 準備建立 Lasso 模型\n",
    "\n",
    "我們接下來可以嘗試使用 Python 建立一個 Lasso 回歸模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e450640-d698-4440-b1fb-1ddd4425bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.45386948 94.67737779 40.55794675  0.         -0.         -0.\n",
      " 10.98839003 95.40225582 80.67610577 34.83198456 -0.          0.\n",
      "  0.         -0.         -0.         29.82848069  7.12581798 -0.\n",
      "  0.         52.17111821]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# 建立模擬資料\n",
    "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# 建立並訓練 Lasso 模型\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 印出模型係數\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa9738e-bae7-43ed-9969-89db71a6c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0. ]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf50eb3-de15-4e04-ab5a-34bb36602e50",
   "metadata": {},
   "source": [
    "* We create an instance of Lasso with the regularization parameter alpha set to 0.1.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14e386-dfc3-486c-b467-a14426ef7659",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef2429-0fd4-449f-819a-951eb92c6a69",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression is a classification method that estimates the probabilities of the possible outcomes using a logistic function. It is commonly used for binary classification tasks. Logistic regression can also be extended to handle multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a15915-b221-418e-8a9c-583d2e53bde9",
   "metadata": {},
   "source": [
    "# Logistic Regression（邏輯回歸）介紹\n",
    "\n",
    "**Logistic Regression** 是一種用於分類任務的統計方法，透過 logistic 函數（S 型函數）將線性回歸輸出轉換為介於 0 與 1 之間的機率值。它最常用於 **二元分類（binary classification）** 問題，例如判斷是否會發生某事件。\n",
    "\n",
    "---\n",
    "\n",
    "## 模型概念\n",
    "\n",
    "Logistic 回歸建模的是事件發生的 **機率**，模型形式為：\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid X) = \\frac{1}{1 + e^{-(X\\beta)}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $P(y = 1 \\mid X)$ 是在給定輸入 $X$ 條件下，$y$ 為 1 的機率  \n",
    "- $X$ 是特徵向量，$\\beta$ 是參數向量  \n",
    "- $e$ 是自然對數的底數  \n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 為 sigmoid 函數\n",
    "\n",
    "---\n",
    "\n",
    "## 特性與擴充\n",
    "\n",
    "- 模型輸出為機率，可根據閾值（通常為 0.5）轉換為類別預測\n",
    "- 可延伸為多分類模型（multinomial logistic regression / softmax regression）\n",
    "- 可加入 L1 或 L2 正則化項進行模型控制\n",
    "\n",
    "---\n",
    "\n",
    "## 與線性回歸的差異\n",
    "\n",
    "| 項目             | 線性回歸               | 邏輯回歸                 |\n",
    "|------------------|------------------------|----------------------------|\n",
    "| 輸出範圍         | 任意實數               | 機率值（0 到 1）            |\n",
    "| 問題類型         | 預測連續值             | 分類問題（機率/類別）       |\n",
    "| 損失函數         | 殘差平方和（MSE）       | 邏輯損失（log loss / cross entropy） |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e8d575-b044-4ce5-a995-cbbe78f5bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 建立模擬資料（binary classification）\n",
    "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
    "\n",
    "# 分割訓練與測試資料\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 建立並訓練模型\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19de345b-344d-46c7-a1b7-d7d3af0ebcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39758121 -0.50380348  0.07252856  1.87977126  0.08573005]]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression(random_state=0).fit(X, y)\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e1e3b-025c-4d80-abf5-877023138b80",
   "metadata": {},
   "source": [
    "* We create an instance of LogisticRegression with the random_state parameter set to 0.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768d3d3-03f4-48f4-a867-ca53bb76a0f5",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent (SGD) is a simple yet efficient approach for training linear models. It is particularly useful when the number of samples and features is very large. SGD updates the model parameters using a small subset of the training data at each iteration, which makes it suitable for online learning and out-of-core learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8bd86-eedf-4fff-bbb7-77414f594f03",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) 介紹\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** 是一種簡單而高效的模型訓練方法，特別適用於樣本數或特徵數非常龐大的情況。與傳統的批次梯度下降（Batch Gradient Descent）不同，SGD 每次僅使用一筆或少量資料來更新參數，這使得它特別適合：\n",
    "\n",
    "- 線上學習（online learning）\n",
    "- 流式資料處理（streaming data）\n",
    "- 超大資料集（out-of-core learning）\n",
    "\n",
    "---\n",
    "\n",
    "## 基本概念\n",
    "\n",
    "對於一個參數為 $\\theta$ 的模型，若損失函數為 $L(\\theta)$，則傳統梯度下降的更新公式為：\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "而 **SGD** 則是使用隨機選取的單筆資料 $(x_i, y_i)$ 來近似損失：\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla L_i(\\theta)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\eta$ 是學習率（learning rate）\n",
    "- $L_i(\\theta)$ 是對樣本 $i$ 的損失\n",
    "\n",
    "這樣的更新速度快、記憶體需求低，但會帶來較高的參數震盪（不穩定性），通常可透過 mini-batch、動態學習率或動量法改善。\n",
    "\n",
    "---\n",
    "\n",
    "## 優點與特性\n",
    "\n",
    "- ⚡ **快速更新**：適合即時學習與模型微調\n",
    "- 📉 **低記憶體使用**：每次只載入少量資料\n",
    "- 🔄 **可持續訓練**：支援 streaming / online learning 模式\n",
    "- ⚠️ **收斂較不穩定**：需調整學習率與其他超參數\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1fa89-1219-4eb5-9b3b-3868d61f839b",
   "metadata": {},
   "source": [
    "## 使用 SGD 訓練 Logistic Regression 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8d1bd9-dcba-4de6-8a6b-696cb2016bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8066666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 建立模擬資料\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# 切分資料\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 建立並訓練 SGD 分類器（使用邏輯斯回歸損失）\n",
    "model = SGDClassifier(loss=\"log_loss\", max_iter=1000, learning_rate='optimal', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9300e5d5-0e91-4b02-b42c-bd401a73d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11068527 -0.86440343  0.71723043 -0.8459125  -0.20550116  2.76668295\n",
      "   0.04900348 -0.39310887 -0.60430234 -0.26534078  0.5088726   0.70344231\n",
      "   0.0689058  -0.01342635 -0.8484677   0.52268594 -0.8268332  -0.49510664\n",
      "  -1.3159977  -1.01810651]]\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(loss=\"log_loss\", max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568702b-4079-40a4-9473-8ed1abbacf68",
   "metadata": {},
   "source": [
    "* We create an instance of SGDClassifier with the loss parameter set to \"log_loss\" to perform logistic regression.\n",
    "* We use the fit method to fit the model to the training data.\n",
    "* We print the coefficients of the logistic regression model obtained using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c2dc5-c8d4-4943-b1f7-7cb8d543e8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
